{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f4522",
   "metadata": {},
   "source": [
    "# 1.Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad45b87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x206d46bcd50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import concurrent.futures\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df274b",
   "metadata": {},
   "source": [
    "# 2.Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889281eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_client_data(data_dir, client_type, idx, img_size=28):\n",
    "    X, y = [], []\n",
    "    folder = os.path.join(data_dir, f\"{client_type}_{idx}\")\n",
    "    for label in os.listdir(folder):\n",
    "        label_folder = os.path.join(folder, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "        for fname in os.listdir(label_folder):\n",
    "            if fname.endswith('.png') or fname.endswith('.jpg'):\n",
    "                img = Image.open(os.path.join(label_folder, fname)).convert('L').resize((img_size, img_size))\n",
    "                X.append(np.array(img).flatten() / 255.0)\n",
    "                y.append(label)\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "def load_mnist_binary_test_data_flat(test_dir, img_size=28):\n",
    "    X = []\n",
    "    y = []\n",
    "    if not os.path.exists(test_dir):\n",
    "        return X, y\n",
    "    for label in ['0', '1']:\n",
    "        label_folder = os.path.join(test_dir, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "        for fname in os.listdir(label_folder):\n",
    "            if fname.endswith('.png') or fname.endswith('.jpg'):\n",
    "                img = Image.open(os.path.join(label_folder, fname)).convert('L').resize((img_size, img_size))\n",
    "                X.append(np.array(img).flatten() / 255.0)\n",
    "                y.append(label)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4c3e",
   "metadata": {},
   "source": [
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=3, padding=1) # 28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 14x14\n",
    "        self.fc1 = nn.Linear(1 * 14 * 14, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# save state_dict + metadata (safe, portable)\n",
    "def save_model(model, img_size, n_classes, learning_rate, experiment_bs, file_path):\n",
    "\n",
    "    state = {\n",
    "        \"model_state_dict\": model.state_dict(),                     # CPU/GPU tensors okay\n",
    "        \"arch\": \"MNISTNet\",\n",
    "        \"img_size\": img_size,\n",
    "        \"num_classes\": n_classes,\n",
    "        \"training_args\": {\"lr\": learning_rate, \"batch_size\": experiment_bs}\n",
    "    }\n",
    "    # ensure weights are on CPU to avoid GPU-only pickle issues\n",
    "    state[\"model_state_dict\"] = {k: v.cpu() for k, v in state[\"model_state_dict\"].items()}\n",
    "    torch.save(state, file_path)\n",
    "    \n",
    "def load_model(file_path, device):\n",
    "\n",
    "    checkpoint = torch.load(file_path, map_location=device)\n",
    "    model = MNISTNet().to(device)              # must have MNISTNet class defined/importable\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e6300",
   "metadata": {},
   "source": [
    "# 4.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd29671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred, target):\n",
    "    eps = 1e-7\n",
    "    pred = torch.clamp(pred, eps, 1 - eps)\n",
    "    return -(target * torch.log(pred) + (1 - target) * torch.log(1 - pred)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def train_local_worker(args):\n",
    "#     X_c_tensor, y_c_tensor, global_weights, learning_rate, local_epoch, experiment_bs, device = args\n",
    "#     model = MNISTNet().to(device)\n",
    "#     model.load_state_dict(global_weights)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     model.train()\n",
    "#     for epoch in range(local_epoch):\n",
    "#         for start in range(0, X_c_tensor.size(0), experiment_bs):\n",
    "#             end = start + experiment_bs\n",
    "#             xb = X_c_tensor[start:end]\n",
    "#             yb = y_c_tensor[start:end]\n",
    "#             outputs = model(xb)\n",
    "#             preds = (outputs >= 0.5).float()\n",
    "#             loss = binary_cross_entropy(outputs, yb)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     # Return a deepcopy to avoid issues with state_dict references\n",
    "#     return copy.deepcopy(model.state_dict()), X_c_tensor.size(0), yb, outputs, preds\n",
    "def train_local_worker(args):\n",
    "    X_c_tensor, y_c_tensor, global_weights, learning_rate, local_epoch, experiment_bs, device = args\n",
    "    # create model on target device\n",
    "    model = MNISTNet().to(device)\n",
    "    # ensure weights are mapped to the worker device\n",
    "    state_on_device = {k: v.to(device) for k, v in global_weights.items()}\n",
    "    model.load_state_dict(state_on_device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(local_epoch):\n",
    "        for start in range(0, X_c_tensor.size(0), experiment_bs):\n",
    "            end = start + experiment_bs\n",
    "            xb = X_c_tensor[start:end].to(device)\n",
    "            yb = y_c_tensor[start:end].to(device)\n",
    "            outputs = model(xb)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            loss = binary_cross_entropy(outputs, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # return CPU copies for safe aggregation in main thread\n",
    "    returned_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "    return returned_state, X_c_tensor.size(0), yb.detach().cpu(), outputs.detach().cpu(), preds.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_weights(w_list):\n",
    "    avg = {}\n",
    "    for k in w_list[0].keys():\n",
    "        avg[k] = sum([w[k] for w in w_list]) / len(w_list)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5df39",
   "metadata": {},
   "source": [
    "# 5.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66763e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HONEST = 100\n",
    "N_POISONED = 100\n",
    "IMG_SIZE = 28\n",
    "LEARNING_RATE = 0.001\n",
    "ROUNDS = 100\n",
    "LOCAL_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "N_CLASSES = 2\n",
    "\n",
    "# Set device\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "DATA_PATH = \"./data/mnist_binary_poison/train\"\n",
    "TEST_PATH = \"./data/mnist_binary_poison/test\"\n",
    "\n",
    "RESULT_PATH = \"./results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888dfd7",
   "metadata": {},
   "source": [
    "# 6.Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886592d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding poison n: 0 for percent: 0\n",
      "Adding poison n: 12 for percent: 10\n",
      "Adding poison n: 25 for percent: 20\n",
      "Adding poison n: 43 for percent: 30\n",
      "Adding poison n: 67 for percent: 40\n",
      "Adding poison n: 100 for percent: 50\n",
      "Poison n list: [0, 12, 25, 43, 67, 100]\n"
     ]
    }
   ],
   "source": [
    "poison_percent = [0, 10, 20, 30, 40, 50]\n",
    "poison_n_list = []\n",
    "for p in range(0,N_POISONED+1):\n",
    "    percent = int((p / (p+N_HONEST)) * 100)\n",
    "    if percent in poison_percent:\n",
    "        poison_percent.remove(percent)\n",
    "        print(f\"Adding poison n: {p} for percent: {percent}\")\n",
    "        poison_n_list.append(p)\n",
    "print(\"Poison n list:\", poison_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c0487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poison_percent 0 round 0: loss_train=0.6549, acc_train=0.6800, loss_test=0.6564, acc_test=0.6658\n",
      "poison_percent 0 round 1: loss_train=0.5856, acc_train=0.8200, loss_test=0.5688, acc_test=0.8888\n",
      "poison_percent 0 round 2: loss_train=0.4389, acc_train=0.9600, loss_test=0.4120, acc_test=0.9628\n",
      "poison_percent 0 round 3: loss_train=0.2373, acc_train=0.9900, loss_test=0.2481, acc_test=0.9862\n",
      "poison_percent 0 round 4: loss_train=0.1421, acc_train=0.9800, loss_test=0.1313, acc_test=0.9939\n",
      "poison_percent 0 round 5: loss_train=0.0655, acc_train=1.0000, loss_test=0.0699, acc_test=0.9954\n",
      "poison_percent 0 round 6: loss_train=0.0466, acc_train=0.9900, loss_test=0.0413, acc_test=0.9949\n",
      "poison_percent 0 round 7: loss_train=0.0392, acc_train=0.9900, loss_test=0.0281, acc_test=0.9949\n",
      "poison_percent 0 round 8: loss_train=0.0184, acc_train=0.9900, loss_test=0.0210, acc_test=0.9954\n",
      "poison_percent 0 round 9: loss_train=0.0463, acc_train=0.9800, loss_test=0.0169, acc_test=0.9954\n",
      "poison_percent 0 round 10: loss_train=0.0279, acc_train=0.9900, loss_test=0.0145, acc_test=0.9954\n",
      "poison_percent 0 round 11: loss_train=0.0197, acc_train=0.9900, loss_test=0.0130, acc_test=0.9954\n",
      "poison_percent 0 round 12: loss_train=0.0306, acc_train=0.9900, loss_test=0.0120, acc_test=0.9954\n",
      "poison_percent 0 round 13: loss_train=0.0107, acc_train=1.0000, loss_test=0.0113, acc_test=0.9954\n",
      "poison_percent 0 round 14: loss_train=0.0495, acc_train=0.9900, loss_test=0.0109, acc_test=0.9954\n",
      "poison_percent 0 round 15: loss_train=0.0249, acc_train=0.9800, loss_test=0.0106, acc_test=0.9954\n",
      "poison_percent 0 round 16: loss_train=0.0846, acc_train=0.9900, loss_test=0.0106, acc_test=0.9954\n",
      "poison_percent 0 round 17: loss_train=0.0321, acc_train=0.9900, loss_test=0.0104, acc_test=0.9954\n",
      "poison_percent 0 round 18: loss_train=0.0627, acc_train=0.9900, loss_test=0.0102, acc_test=0.9954\n",
      "poison_percent 0 round 19: loss_train=0.0033, acc_train=1.0000, loss_test=0.0101, acc_test=0.9954\n",
      "poison_percent 0 round 20: loss_train=0.0017, acc_train=1.0000, loss_test=0.0102, acc_test=0.9954\n",
      "poison_percent 0 round 21: loss_train=0.0393, acc_train=0.9900, loss_test=0.0103, acc_test=0.9954\n",
      "poison_percent 0 round 22: loss_train=0.0002, acc_train=1.0000, loss_test=0.0103, acc_test=0.9954\n",
      "poison_percent 0 round 23: loss_train=0.0047, acc_train=1.0000, loss_test=0.0103, acc_test=0.9954\n",
      "poison_percent 0 round 24: loss_train=0.0302, acc_train=0.9900, loss_test=0.0103, acc_test=0.9954\n",
      "poison_percent 0 round 25: loss_train=0.0286, acc_train=0.9900, loss_test=0.0107, acc_test=0.9954\n",
      "poison_percent 0 round 26: loss_train=0.0351, acc_train=0.9900, loss_test=0.0107, acc_test=0.9954\n",
      "poison_percent 0 round 27: loss_train=0.0021, acc_train=1.0000, loss_test=0.0108, acc_test=0.9954\n",
      "poison_percent 0 round 28: loss_train=0.0142, acc_train=0.9900, loss_test=0.0110, acc_test=0.9954\n",
      "poison_percent 0 round 29: loss_train=0.0243, acc_train=0.9900, loss_test=0.0109, acc_test=0.9954\n",
      "poison_percent 0 round 30: loss_train=0.0069, acc_train=1.0000, loss_test=0.0113, acc_test=0.9954\n",
      "poison_percent 0 round 31: loss_train=0.0346, acc_train=0.9900, loss_test=0.0115, acc_test=0.9954\n",
      "poison_percent 0 round 32: loss_train=0.1647, acc_train=0.9800, loss_test=0.0115, acc_test=0.9954\n",
      "poison_percent 0 round 33: loss_train=0.0425, acc_train=0.9900, loss_test=0.0116, acc_test=0.9954\n",
      "poison_percent 0 round 34: loss_train=0.1064, acc_train=0.9900, loss_test=0.0116, acc_test=0.9954\n",
      "poison_percent 0 round 35: loss_train=0.0088, acc_train=0.9900, loss_test=0.0117, acc_test=0.9954\n",
      "poison_percent 0 round 36: loss_train=0.0087, acc_train=1.0000, loss_test=0.0117, acc_test=0.9954\n",
      "poison_percent 0 round 37: loss_train=0.0000, acc_train=1.0000, loss_test=0.0121, acc_test=0.9954\n",
      "poison_percent 0 round 38: loss_train=0.0000, acc_train=1.0000, loss_test=0.0119, acc_test=0.9954\n",
      "poison_percent 0 round 39: loss_train=0.0368, acc_train=0.9900, loss_test=0.0121, acc_test=0.9954\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Run in parallel\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=\u001b[32m2\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     honest_results = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_local_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhonest_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     poison_results = \u001b[38;5;28mlist\u001b[39m(executor.map(train_local_worker, poison_args))\n\u001b[32m    108\u001b[39m local_weights = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE_list = []\n",
    "temp_bs = BATCH_SIZE\n",
    "while temp_bs >= 1:\n",
    "    BATCH_SIZE_list.append(temp_bs)\n",
    "    temp_bs = temp_bs // 2\n",
    "\n",
    "for rev in range(3):\n",
    "\n",
    "    global_model = MNISTNet().to(device)\n",
    "    for experiment_bs in BATCH_SIZE_list:\n",
    "\n",
    "        name_save_path = f\"FL_ModelN1L1_Batchsize{experiment_bs}_rev{rev}\"\n",
    "        \n",
    "        for i_poisoned in poison_n_list:\n",
    "            percent_poisoned = int((i_poisoned / (i_poisoned + N_HONEST)) * 100)\n",
    "\n",
    "            # create directory\n",
    "            NAME_SAVE_update_PATH = f\"poisoned_{percent_poisoned}percent\"\n",
    "            save_path = os.path.join(RESULT_PATH,name_save_path,NAME_SAVE_update_PATH)\n",
    "            # Remove existing directory if it exists\n",
    "            if os.path.exists(save_path):\n",
    "                shutil.rmtree(save_path)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "            save_all_path = os.path.join(RESULT_PATH,name_save_path,'all')\n",
    "            os.makedirs(save_all_path, exist_ok=True)\n",
    "            \n",
    "            # Load data\n",
    "            X_test, y_test = load_mnist_binary_test_data_flat(TEST_PATH, img_size=IMG_SIZE)\n",
    "            if X_test.shape[0] == 0:\n",
    "                raise ValueError(\"No valid test images found in the test directory!\")\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            # torch.save(model, os.path.join(save_path,'model_init.pt'))\n",
    "            save_path_name = os.path.join(save_path,'model_init.pt')\n",
    "            save_model(global_model, IMG_SIZE, N_CLASSES, LEARNING_RATE, experiment_bs, save_path_name)\n",
    "            \n",
    "            global_weights = global_model.state_dict()\n",
    "\n",
    "            records = []\n",
    "\n",
    "            X_train_tensor = {}\n",
    "            y_train_tensor = {}\n",
    "            for i in range(N_HONEST):\n",
    "                X_c, y_c = get_client_data(DATA_PATH, \"honest\", i, img_size=IMG_SIZE)\n",
    "                if len(X_c) == 0:\n",
    "                    continue\n",
    "                X_c_tensor = torch.tensor(X_c, dtype=torch.float32).to(device)\n",
    "                y_c_tensor = torch.tensor(y_c, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                X_train_tensor[f\"honest_{i}\"] = X_c_tensor\n",
    "                y_train_tensor[f\"honest_{i}\"] = y_c_tensor\n",
    "            for i in range(i_poisoned):\n",
    "                X_c, y_c = get_client_data(DATA_PATH, \"poison\", i, img_size=IMG_SIZE)\n",
    "                if len(X_c) == 0:\n",
    "                    continue\n",
    "                X_c_tensor = torch.tensor(X_c, dtype=torch.float32).to(device)\n",
    "                y_c_tensor = torch.tensor(y_c, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                X_train_tensor[f\"poison_{i}\"] = X_c_tensor\n",
    "                y_train_tensor[f\"poison_{i}\"] = y_c_tensor\n",
    "\n",
    "            # Training loop\n",
    "            for round in range(ROUNDS):\n",
    "                # Prepare arguments for honest clients\n",
    "                \n",
    "                #randomize the training data\n",
    "                for i in range(N_HONEST):\n",
    "                    perm = torch.randperm(X_train_tensor[f\"honest_{i}\"].size(0))\n",
    "                    X_train_tensor[f\"honest_{i}\"] = X_train_tensor[f\"honest_{i}\"][perm]\n",
    "                    y_train_tensor[f\"honest_{i}\"] = y_train_tensor[f\"honest_{i}\"][perm]\n",
    "\n",
    "                honest_args = [\n",
    "                    (\n",
    "                        X_train_tensor[f\"honest_{i}\"],\n",
    "                        y_train_tensor[f\"honest_{i}\"],\n",
    "                        global_weights,\n",
    "                        LEARNING_RATE,\n",
    "                        LOCAL_EPOCHS,\n",
    "                        experiment_bs,\n",
    "                        device\n",
    "                    )\n",
    "                    for i in range(N_HONEST)\n",
    "                ]\n",
    "\n",
    "                # Prepare arguments for poisoned clients\n",
    "                \n",
    "                #randomize the training data\n",
    "                for i in range(i_poisoned):\n",
    "                    perm = torch.randperm(X_train_tensor[f\"poison_{i}\"].size(0))\n",
    "                    X_train_tensor[f\"poison_{i}\"] = X_train_tensor[f\"poison_{i}\"][perm]\n",
    "                    y_train_tensor[f\"poison_{i}\"] = y_train_tensor[f\"poison_{i}\"][perm]\n",
    "\n",
    "                poison_args = [\n",
    "                    (\n",
    "                        X_train_tensor[f\"poison_{i}\"],\n",
    "                        y_train_tensor[f\"poison_{i}\"],\n",
    "                        global_weights,\n",
    "                        LEARNING_RATE,\n",
    "                        LOCAL_EPOCHS,\n",
    "                        experiment_bs,\n",
    "                        device\n",
    "                    )\n",
    "                    for i in range(i_poisoned)\n",
    "                ]\n",
    "\n",
    "                # Run in parallel\n",
    "                # with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "                #     honest_results = list(executor.map(train_local_worker, honest_args))\n",
    "                #     poison_results = list(executor.map(train_local_worker, poison_args))\n",
    "                \n",
    "                # combine args so executor schedules them together\n",
    "                all_args = honest_args + poison_args\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "                    results = list(executor.map(train_local_worker, all_args))\n",
    "                # then unpack results (results is in same order as all_args)\n",
    "\n",
    "                local_weights = []\n",
    "                local_sizes = []\n",
    "                local_y = []\n",
    "                local_outputs = []\n",
    "                local_preds = []\n",
    "                for l_w, l_sz, l_y, l_out, l_pred in results:\n",
    "                    local_weights.append(l_w)\n",
    "                    local_sizes.append(l_sz)\n",
    "                    local_y.extend(l_y)\n",
    "                    local_outputs.extend(l_out)\n",
    "                    local_preds.extend(l_pred)\n",
    "                loss_train = binary_cross_entropy(torch.stack(local_outputs), torch.stack(local_y))\n",
    "                acc_train = (torch.stack(local_preds) == torch.stack(local_y)).float().mean().item()\n",
    "\n",
    "\n",
    "                # Federated averaging (weighted by client data size)\n",
    "                new_weights = {}\n",
    "                for key in global_weights.keys():\n",
    "                    new_weights[key] = sum([w[key]*sz for w, sz in zip(local_weights, local_sizes)]) / sum(local_sizes)\n",
    "                global_weights = new_weights\n",
    "                global_model.load_state_dict(global_weights)\n",
    "\n",
    "                # Evaluate\n",
    "                global_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = global_model(X_test_tensor)\n",
    "                    preds = (outputs > 0.5).float()\n",
    "                    loss_test = binary_cross_entropy(outputs, y_test_tensor)\n",
    "                    acc_test = (preds == y_test_tensor).float().mean().item()\n",
    "                records.append({'poison_percent': percent_poisoned, 'round': round, \n",
    "                                'loss_train': loss_train.item(), 'acc_train': acc_train, \n",
    "                                'loss_test': loss_test.item(), 'acc_test': acc_test})\n",
    "                report_txt = f\"poison_percent {percent_poisoned} \"\n",
    "                report_txt += f\"round {round}: \"\n",
    "                report_txt += f\"loss_train={loss_train.item():.4f}, \"\n",
    "                report_txt += f\"acc_train={acc_train:.4f}, \"\n",
    "                report_txt += f\"loss_test={loss_test.item():.4f}, \"\n",
    "                report_txt += f\"acc_test={acc_test:.4f}\"\n",
    "                print(report_txt)\n",
    "\n",
    "            # torch.save(model, os.path.join(save_path,'model_last.pt'))\n",
    "            save_path_name = os.path.join(save_path,'model_last.pt')\n",
    "            save_model(global_model, IMG_SIZE, N_CLASSES, LEARNING_RATE, experiment_bs, save_path_name)\n",
    "\n",
    "            # Save training log\n",
    "            df = pd.DataFrame(records)\n",
    "            save_name_path = os.path.join(save_path, f'{NAME_SAVE_update_PATH}.csv')\n",
    "            df.to_csv(save_name_path, index=False)\n",
    "            print(f\"Training log saved to {save_name_path}\")\n",
    "\n",
    "            # Plot loss and accuracy\n",
    "            plt.figure(figsize=(10,4))\n",
    "\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(df['round'], df['loss_test'], marker='o')\n",
    "            plt.title('Test Loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.grid(True)\n",
    "            plt.ylim(0, 1.1)\n",
    "            # plt.legend()\n",
    "\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(df['round'], df['acc_test'], marker='o')\n",
    "            plt.title('Test Accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.grid(True)\n",
    "            plt.ylim(0, 1.1)\n",
    "            # plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            save_name_path = os.path.join(save_path, f'loss_accuracy.jpg')\n",
    "            plt.savefig(save_name_path)\n",
    "\n",
    "            save_name_path = os.path.join(save_all_path, f'{NAME_SAVE_update_PATH}_latest.jpg')\n",
    "            plt.savefig(save_name_path)\n",
    "            plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
