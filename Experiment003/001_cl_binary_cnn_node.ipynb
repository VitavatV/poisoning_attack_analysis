{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f4522",
   "metadata": {},
   "source": [
    "# 1.Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad45b87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20905da9db0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df274b",
   "metadata": {},
   "source": [
    "# 2.Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "889281eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_binary_data(data_dir, honest_range=100, poison_range=100, img_size=28,select='Honest+Poisoned'):\n",
    "    X = []\n",
    "    y = []\n",
    "    def load_honest(X, y):\n",
    "        for i in range(honest_range):\n",
    "            folder = os.path.join(data_dir, f\"honest_{i}\")\n",
    "            if not os.path.exists(folder):\n",
    "                continue\n",
    "            for label in os.listdir(folder):\n",
    "                label_folder = os.path.join(folder, label)\n",
    "                if not os.path.isdir(label_folder):\n",
    "                    continue\n",
    "                for fname in os.listdir(label_folder):\n",
    "                    if fname.endswith('.png') or fname.endswith('.jpg'):\n",
    "                        img = Image.open(os.path.join(label_folder, fname)).convert('L').resize((img_size, img_size))\n",
    "                        X.append(np.array(img).flatten() / 255.0)\n",
    "                        y.append(label)\n",
    "        return X, y\n",
    "    def load_poisoned(X, y):\n",
    "        for i in range(poison_range):\n",
    "            folder = os.path.join(data_dir, f\"poison_{i}\")\n",
    "            if not os.path.exists(folder):\n",
    "                continue\n",
    "            for label in os.listdir(folder):\n",
    "                label_folder = os.path.join(folder, label)\n",
    "                if not os.path.isdir(label_folder):\n",
    "                    continue\n",
    "                for fname in os.listdir(label_folder):\n",
    "                    if fname.endswith('.png') or fname.endswith('.jpg'):\n",
    "                        img = Image.open(os.path.join(label_folder, fname)).convert('L').resize((img_size, img_size))\n",
    "                        X.append(np.array(img).flatten() / 255.0)\n",
    "                        y.append(label)\n",
    "        return X, y\n",
    "    \n",
    "    if select == 'Honest+Poisoned':\n",
    "        X, y = load_honest(X, y)\n",
    "        X, y = load_poisoned(X, y)\n",
    "    elif select == 'Poisoned+Honest':\n",
    "        X, y = load_poisoned(X, y)\n",
    "        X, y = load_honest(X, y)\n",
    "    elif select == 'Honest':\n",
    "        X, y = load_honest(X, y)\n",
    "    elif select == 'Poisoned':\n",
    "        X, y = load_poisoned(X, y)\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "def load_mnist_binary_test_data(test_dir, img_size=28):\n",
    "    X = []\n",
    "    y = []\n",
    "    if not os.path.exists(test_dir):\n",
    "        return X, y\n",
    "    for label in ['0', '1']:\n",
    "        label_folder = os.path.join(test_dir, label)\n",
    "        if not os.path.isdir(label_folder):\n",
    "            continue\n",
    "        for fname in os.listdir(label_folder):\n",
    "            if fname.endswith('.png') or fname.endswith('.jpg'):\n",
    "                img = Image.open(os.path.join(label_folder, fname)).convert('L').resize((img_size, img_size))\n",
    "                X.append(np.array(img).flatten() / 255.0)\n",
    "                y.append(label)\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a4c3e",
   "metadata": {},
   "source": [
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40aa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MNISTNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(28*28, 32)\n",
    "#         self.fc2 = nn.Linear(32, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fcd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MNISTNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(28*28, 1)\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#         return x\n",
    "    \n",
    "# save state_dict + metadata (safe, portable)\n",
    "def save_model(model, img_size, n_classes, learning_rate, experiment_bs, file_path):\n",
    "\n",
    "    state = {\n",
    "        \"model_state_dict\": model.state_dict(),                     # CPU/GPU tensors okay\n",
    "        \"arch\": \"MNISTNet\",\n",
    "        \"img_size\": img_size,\n",
    "        \"num_classes\": n_classes,\n",
    "        \"training_args\": {\"lr\": learning_rate, \"batch_size\": experiment_bs}\n",
    "    }\n",
    "    # ensure weights are on CPU to avoid GPU-only pickle issues\n",
    "    state[\"model_state_dict\"] = {k: v.cpu() for k, v in state[\"model_state_dict\"].items()}\n",
    "    torch.save(state, file_path)\n",
    "    \n",
    "def load_model(file_path, device):\n",
    "\n",
    "    checkpoint = torch.load(file_path, map_location=device)\n",
    "    model = MNISTNet().to(device)              # must have MNISTNet class defined/importable\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e6300",
   "metadata": {},
   "source": [
    "# 4.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd29671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred, target):\n",
    "    eps = 1e-7\n",
    "    pred = torch.clamp(pred, eps, 1 - eps)\n",
    "    return -(target * torch.log(pred) + (1 - target) * torch.log(1 - pred)).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5df39",
   "metadata": {},
   "source": [
    "# 5.Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66763e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HONEST = 100\n",
    "N_POISONED = 100\n",
    "IMG_SIZE = 28\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE =32\n",
    "N_CLASSES = 2\n",
    "\n",
    "# Set device\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "DATA_PATH = \"./data/mnist_binary_poison/train\"\n",
    "TEST_PATH = \"./data/mnist_binary_poison/test\"\n",
    "\n",
    "RESULT_PATH = \"./results\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888dfd7",
   "metadata": {},
   "source": [
    "# 6.Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886592d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding poison n: 0 for percent: 0\n",
      "Adding poison n: 12 for percent: 10\n",
      "Adding poison n: 25 for percent: 20\n",
      "Adding poison n: 43 for percent: 30\n",
      "Adding poison n: 67 for percent: 40\n",
      "Adding poison n: 100 for percent: 50\n",
      "Poison n list: [0, 12, 25, 43, 67, 100]\n"
     ]
    }
   ],
   "source": [
    "poison_percent = [0, 10, 20, 30, 40, 50]\n",
    "poison_n_list = []\n",
    "for p in range(0,N_POISONED+1):\n",
    "    percent = int((p / (p+N_HONEST)) * 100)\n",
    "    if percent in poison_percent:\n",
    "        poison_percent.remove(percent)\n",
    "        print(f\"Adding poison n: {p} for percent: {percent}\")\n",
    "        poison_n_list.append(p)\n",
    "print(\"Poison n list:\", poison_n_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c0487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poison_percent 0 epoch 0: loss_train=0.0133, acc_train=1.0000, loss_test=0.0025, acc_test=0.9995\n",
      "poison_percent 0 epoch 1: loss_train=0.0019, acc_train=1.0000, loss_test=0.0020, acc_test=0.9990\n",
      "poison_percent 0 epoch 2: loss_train=0.0000, acc_train=1.0000, loss_test=0.0018, acc_test=0.9990\n",
      "poison_percent 0 epoch 3: loss_train=0.0000, acc_train=1.0000, loss_test=0.0019, acc_test=0.9990\n",
      "poison_percent 0 epoch 4: loss_train=0.0000, acc_train=1.0000, loss_test=0.0013, acc_test=0.9995\n",
      "poison_percent 0 epoch 5: loss_train=0.0000, acc_train=1.0000, loss_test=0.0011, acc_test=0.9995\n",
      "poison_percent 0 epoch 6: loss_train=0.0001, acc_train=1.0000, loss_test=0.0011, acc_test=1.0000\n",
      "poison_percent 0 epoch 7: loss_train=0.0000, acc_train=1.0000, loss_test=0.0014, acc_test=0.9995\n",
      "poison_percent 0 epoch 8: loss_train=0.0000, acc_train=1.0000, loss_test=0.0029, acc_test=0.9995\n",
      "poison_percent 0 epoch 9: loss_train=0.0000, acc_train=1.0000, loss_test=0.0059, acc_test=0.9980\n",
      "poison_percent 0 epoch 10: loss_train=0.0000, acc_train=1.0000, loss_test=0.0021, acc_test=0.9990\n",
      "poison_percent 0 epoch 11: loss_train=0.0000, acc_train=1.0000, loss_test=0.0018, acc_test=0.9990\n",
      "poison_percent 0 epoch 12: loss_train=0.0000, acc_train=1.0000, loss_test=0.0029, acc_test=0.9990\n",
      "poison_percent 0 epoch 13: loss_train=0.0000, acc_train=1.0000, loss_test=0.0022, acc_test=0.9990\n",
      "poison_percent 0 epoch 14: loss_train=0.0000, acc_train=1.0000, loss_test=0.0032, acc_test=0.9990\n",
      "poison_percent 0 epoch 15: loss_train=0.0000, acc_train=1.0000, loss_test=0.0025, acc_test=0.9995\n",
      "poison_percent 0 epoch 16: loss_train=0.0000, acc_train=1.0000, loss_test=0.0023, acc_test=0.9995\n",
      "poison_percent 0 epoch 17: loss_train=0.0149, acc_train=1.0000, loss_test=0.0020, acc_test=0.9995\n",
      "poison_percent 0 epoch 18: loss_train=0.0000, acc_train=1.0000, loss_test=0.0019, acc_test=0.9990\n",
      "poison_percent 0 epoch 19: loss_train=0.0000, acc_train=1.0000, loss_test=0.0023, acc_test=0.9995\n",
      "poison_percent 0 epoch 20: loss_train=0.0000, acc_train=1.0000, loss_test=0.0020, acc_test=0.9995\n",
      "poison_percent 0 epoch 21: loss_train=0.0000, acc_train=1.0000, loss_test=0.0021, acc_test=0.9995\n",
      "poison_percent 0 epoch 22: loss_train=0.0000, acc_train=1.0000, loss_test=0.0033, acc_test=0.9990\n",
      "poison_percent 0 epoch 23: loss_train=0.0000, acc_train=1.0000, loss_test=0.0029, acc_test=0.9985\n",
      "poison_percent 0 epoch 24: loss_train=0.0000, acc_train=1.0000, loss_test=0.0018, acc_test=0.9995\n",
      "poison_percent 0 epoch 25: loss_train=0.0000, acc_train=1.0000, loss_test=0.0007, acc_test=1.0000\n",
      "poison_percent 0 epoch 26: loss_train=0.0000, acc_train=1.0000, loss_test=0.0014, acc_test=0.9990\n",
      "poison_percent 0 epoch 27: loss_train=0.0000, acc_train=1.0000, loss_test=0.0017, acc_test=0.9995\n",
      "poison_percent 0 epoch 28: loss_train=0.0000, acc_train=1.0000, loss_test=0.0022, acc_test=0.9995\n",
      "poison_percent 0 epoch 29: loss_train=0.0000, acc_train=1.0000, loss_test=0.0020, acc_test=0.9995\n",
      "poison_percent 0 epoch 30: loss_train=0.0000, acc_train=1.0000, loss_test=0.0020, acc_test=0.9995\n",
      "poison_percent 0 epoch 31: loss_train=0.0000, acc_train=1.0000, loss_test=0.0030, acc_test=0.9990\n",
      "poison_percent 0 epoch 32: loss_train=0.0000, acc_train=1.0000, loss_test=0.0009, acc_test=1.0000\n",
      "poison_percent 0 epoch 33: loss_train=0.0000, acc_train=1.0000, loss_test=0.0006, acc_test=1.0000\n",
      "poison_percent 0 epoch 34: loss_train=0.0000, acc_train=1.0000, loss_test=0.0009, acc_test=0.9995\n",
      "poison_percent 0 epoch 35: loss_train=0.0000, acc_train=1.0000, loss_test=0.0037, acc_test=0.9990\n",
      "poison_percent 0 epoch 36: loss_train=0.0000, acc_train=1.0000, loss_test=0.0024, acc_test=0.9990\n",
      "poison_percent 0 epoch 37: loss_train=0.0000, acc_train=1.0000, loss_test=0.0042, acc_test=0.9985\n",
      "poison_percent 0 epoch 38: loss_train=0.0000, acc_train=1.0000, loss_test=0.0007, acc_test=0.9995\n",
      "poison_percent 0 epoch 39: loss_train=0.0000, acc_train=1.0000, loss_test=0.0010, acc_test=0.9990\n",
      "poison_percent 0 epoch 40: loss_train=0.0000, acc_train=1.0000, loss_test=0.0040, acc_test=0.9985\n",
      "poison_percent 0 epoch 41: loss_train=0.0000, acc_train=1.0000, loss_test=0.0012, acc_test=0.9995\n",
      "poison_percent 0 epoch 42: loss_train=0.0000, acc_train=1.0000, loss_test=0.0014, acc_test=0.9995\n",
      "poison_percent 0 epoch 43: loss_train=0.0000, acc_train=1.0000, loss_test=0.0020, acc_test=0.9985\n",
      "poison_percent 0 epoch 44: loss_train=0.0000, acc_train=1.0000, loss_test=0.0016, acc_test=0.9995\n",
      "poison_percent 0 epoch 45: loss_train=0.0000, acc_train=1.0000, loss_test=0.0017, acc_test=0.9990\n",
      "poison_percent 0 epoch 46: loss_train=0.0000, acc_train=1.0000, loss_test=0.0068, acc_test=0.9985\n",
      "poison_percent 0 epoch 47: loss_train=0.0000, acc_train=1.0000, loss_test=0.0020, acc_test=0.9985\n",
      "poison_percent 0 epoch 48: loss_train=0.0000, acc_train=1.0000, loss_test=0.0011, acc_test=0.9995\n",
      "poison_percent 0 epoch 49: loss_train=0.0000, acc_train=1.0000, loss_test=0.0017, acc_test=0.9990\n",
      "poison_percent 0 epoch 50: loss_train=0.0000, acc_train=1.0000, loss_test=0.0022, acc_test=0.9985\n",
      "poison_percent 0 epoch 51: loss_train=0.0000, acc_train=1.0000, loss_test=0.0019, acc_test=0.9995\n",
      "poison_percent 0 epoch 52: loss_train=0.0000, acc_train=1.0000, loss_test=0.0019, acc_test=0.9990\n",
      "poison_percent 0 epoch 53: loss_train=0.0000, acc_train=1.0000, loss_test=0.0029, acc_test=0.9985\n",
      "poison_percent 0 epoch 54: loss_train=0.0000, acc_train=1.0000, loss_test=0.0034, acc_test=0.9985\n",
      "poison_percent 0 epoch 55: loss_train=0.0000, acc_train=1.0000, loss_test=0.0025, acc_test=0.9985\n",
      "poison_percent 0 epoch 56: loss_train=0.0000, acc_train=1.0000, loss_test=0.0074, acc_test=0.9985\n",
      "poison_percent 0 epoch 57: loss_train=0.0000, acc_train=1.0000, loss_test=0.0038, acc_test=0.9980\n",
      "poison_percent 0 epoch 58: loss_train=0.0000, acc_train=1.0000, loss_test=0.0021, acc_test=0.9990\n",
      "poison_percent 0 epoch 59: loss_train=0.0000, acc_train=1.0000, loss_test=0.0043, acc_test=0.9990\n",
      "poison_percent 0 epoch 60: loss_train=0.0000, acc_train=1.0000, loss_test=0.0041, acc_test=0.9990\n",
      "poison_percent 0 epoch 61: loss_train=0.0000, acc_train=1.0000, loss_test=0.0086, acc_test=0.9990\n",
      "poison_percent 0 epoch 62: loss_train=0.0000, acc_train=1.0000, loss_test=0.0046, acc_test=0.9985\n",
      "poison_percent 0 epoch 63: loss_train=0.0000, acc_train=1.0000, loss_test=0.0026, acc_test=0.9990\n",
      "poison_percent 0 epoch 64: loss_train=0.0000, acc_train=1.0000, loss_test=0.0027, acc_test=0.9990\n",
      "poison_percent 0 epoch 65: loss_train=0.0000, acc_train=1.0000, loss_test=0.0038, acc_test=0.9990\n",
      "poison_percent 0 epoch 66: loss_train=0.0000, acc_train=1.0000, loss_test=0.0022, acc_test=0.9990\n",
      "poison_percent 0 epoch 67: loss_train=0.0000, acc_train=1.0000, loss_test=0.0026, acc_test=0.9985\n",
      "poison_percent 0 epoch 68: loss_train=0.0000, acc_train=1.0000, loss_test=0.0021, acc_test=0.9995\n",
      "poison_percent 0 epoch 69: loss_train=0.0000, acc_train=1.0000, loss_test=0.0053, acc_test=0.9985\n",
      "poison_percent 0 epoch 70: loss_train=0.0000, acc_train=1.0000, loss_test=0.0027, acc_test=0.9995\n",
      "poison_percent 0 epoch 71: loss_train=0.0000, acc_train=1.0000, loss_test=0.0021, acc_test=0.9995\n",
      "poison_percent 0 epoch 72: loss_train=0.0000, acc_train=1.0000, loss_test=0.0026, acc_test=0.9990\n",
      "poison_percent 0 epoch 73: loss_train=0.0000, acc_train=1.0000, loss_test=0.0156, acc_test=0.9985\n",
      "poison_percent 0 epoch 74: loss_train=0.0000, acc_train=1.0000, loss_test=0.0034, acc_test=0.9985\n",
      "poison_percent 0 epoch 75: loss_train=0.0000, acc_train=1.0000, loss_test=0.0025, acc_test=0.9995\n",
      "poison_percent 0 epoch 76: loss_train=0.0000, acc_train=1.0000, loss_test=0.0036, acc_test=0.9985\n",
      "poison_percent 0 epoch 77: loss_train=0.0000, acc_train=1.0000, loss_test=0.0015, acc_test=0.9995\n",
      "poison_percent 0 epoch 78: loss_train=0.0000, acc_train=1.0000, loss_test=0.0052, acc_test=0.9990\n",
      "poison_percent 0 epoch 79: loss_train=0.0000, acc_train=1.0000, loss_test=0.0035, acc_test=0.9990\n",
      "poison_percent 0 epoch 80: loss_train=0.0000, acc_train=1.0000, loss_test=0.0027, acc_test=0.9995\n",
      "poison_percent 0 epoch 81: loss_train=0.0000, acc_train=1.0000, loss_test=0.0028, acc_test=0.9990\n",
      "poison_percent 0 epoch 82: loss_train=0.0000, acc_train=1.0000, loss_test=0.0040, acc_test=0.9985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     85\u001b[39m     loss_train = binary_cross_entropy(outputs, yb)\n\u001b[32m     86\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[43mloss_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     optimizer.step()\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for NODE in [1,2,4,8,16,32]:\n",
    "    \n",
    "    class MNISTNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv2d(1, NODE, kernel_size=3, padding=1) # 28x28\n",
    "            self.pool = nn.MaxPool2d(2, 2) # 14x14\n",
    "            self.fc1 = nn.Linear(NODE * 14 * 14, 1)\n",
    "            self.relu = nn.ReLU()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "            x = self.pool(self.relu(self.conv1(x)))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc1(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "\n",
    "    BATCH_SIZE_list = []\n",
    "    temp_bs = BATCH_SIZE\n",
    "    while temp_bs >= 1:\n",
    "        BATCH_SIZE_list.append(temp_bs)\n",
    "        temp_bs = temp_bs // 2\n",
    "\n",
    "    for rev in range(3):\n",
    "\n",
    "        model = MNISTNet().to(device)\n",
    "        for experiment_bs in BATCH_SIZE_list:\n",
    "\n",
    "            name_save_path = f\"CL_ModelN{NODE}L2_Batchsize{experiment_bs}_rev{rev}\"\n",
    "            \n",
    "            for i_poisoned in poison_n_list:\n",
    "                percent_poisoned = int((i_poisoned / (i_poisoned + N_HONEST)) * 100)\n",
    "\n",
    "                # create directory\n",
    "                NAME_SAVE_update_PATH = f\"poisoned_{percent_poisoned}percent\"\n",
    "                save_path = os.path.join(RESULT_PATH,name_save_path,NAME_SAVE_update_PATH)\n",
    "                # Remove existing directory if it exists\n",
    "                if os.path.exists(save_path):\n",
    "                    shutil.rmtree(save_path)\n",
    "                if not os.path.exists(save_path):\n",
    "                    os.makedirs(save_path)\n",
    "                save_all_path = os.path.join(RESULT_PATH,name_save_path,'all')\n",
    "                os.makedirs(save_all_path, exist_ok=True)\n",
    "                \n",
    "                # Load data\n",
    "                X_train, y_train = load_mnist_binary_data( DATA_PATH, honest_range=N_HONEST, \n",
    "                                                        poison_range=i_poisoned, img_size=IMG_SIZE, \n",
    "                                                        select='Honest+Poisoned')\n",
    "                X_test, y_test = load_mnist_binary_test_data( TEST_PATH, img_size=IMG_SIZE)\n",
    "                # print(f\"Loaded train: X={X_train.shape}, y={y_train.shape}\")\n",
    "                # print(f\"Loaded test: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "                if X_test.shape[0] == 0:\n",
    "                    raise ValueError(\"No valid test images found in the test directory!\")\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "                y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "                # torch.save(model, os.path.join(save_path,'model_init.pt'))\n",
    "                save_path_name = os.path.join(save_path,'model_init.pt')\n",
    "                save_model(model, IMG_SIZE, N_CLASSES, LEARNING_RATE, experiment_bs, save_path_name)\n",
    "                \n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "                records = []\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(EPOCHS):\n",
    "\n",
    "                    # Train\n",
    "                    model.train()\n",
    "                    \n",
    "                    #randomize the training data\n",
    "                    perm = torch.randperm(X_train_tensor.size(0))\n",
    "                    X_train_tensor = X_train_tensor[perm]\n",
    "                    y_train_tensor = y_train_tensor[perm]\n",
    "\n",
    "                    for start in range(0, X_train_tensor.size(0), experiment_bs):\n",
    "                        end = start + experiment_bs\n",
    "                        xb = X_train_tensor[start:end]\n",
    "                        yb = y_train_tensor[start:end]\n",
    "                        outputs = model(xb)\n",
    "                        preds = (outputs >= 0.5).float()\n",
    "                        acc_train = (preds == yb).float().mean().item()\n",
    "                        loss_train = binary_cross_entropy(outputs, yb)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss_train.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Evaluate\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(X_test_tensor)\n",
    "                        preds = (outputs >= 0.5).float()\n",
    "                        loss_test = binary_cross_entropy(outputs, y_test_tensor)\n",
    "                        acc_test = (preds == y_test_tensor).float().mean().item()\n",
    "                    records.append({'poison_percent': percent_poisoned, 'epoch': epoch, \n",
    "                                    'loss_train': loss_train.item(), 'acc_train': acc_train, \n",
    "                                    'loss_test': loss_test.item(), 'acc_test': acc_test})\n",
    "                    report_txt = f\"poison_percent {percent_poisoned} \"\n",
    "                    report_txt += f\"epoch {epoch}: \"\n",
    "                    report_txt += f\"loss_train={loss_train.item():.4f}, \"\n",
    "                    report_txt += f\"acc_train={acc_train:.4f}, \"\n",
    "                    report_txt += f\"loss_test={loss_test.item():.4f}, \"\n",
    "                    report_txt += f\"acc_test={acc_test:.4f}\"\n",
    "                    print(report_txt)\n",
    "\n",
    "                # torch.save(model, os.path.join(save_path,'model_last.pt'))\n",
    "                save_path_name = os.path.join(save_path,'model_last.pt')\n",
    "                save_model(model, IMG_SIZE, N_CLASSES, LEARNING_RATE, experiment_bs, save_path_name)\n",
    "\n",
    "                # Save training log\n",
    "                df = pd.DataFrame(records)\n",
    "                save_name_path = os.path.join(save_path, f'{NAME_SAVE_update_PATH}.csv')\n",
    "                df.to_csv(save_name_path, index=False)\n",
    "                print(f\"Training log saved to {save_name_path}\")\n",
    "\n",
    "                # Plot loss and accuracy\n",
    "                plt.figure(figsize=(10,4))\n",
    "\n",
    "                plt.subplot(1,2,1)\n",
    "                plt.plot(df['epoch'], df['loss_test'], marker='o')\n",
    "                plt.title('Test Loss')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.grid(True)\n",
    "                plt.ylim(0, 1.1)\n",
    "                # plt.legend()\n",
    "\n",
    "                plt.subplot(1,2,2)\n",
    "                plt.plot(df['epoch'], df['acc_test'], marker='o')\n",
    "                plt.title('Test Accuracy')\n",
    "                plt.ylabel('Accuracy')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.grid(True)\n",
    "                plt.ylim(0, 1.1)\n",
    "                # plt.legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                save_name_path = os.path.join(save_path, f'loss_accuracy.jpg')\n",
    "                plt.savefig(save_name_path)\n",
    "\n",
    "                save_name_path = os.path.join(save_all_path, f'{NAME_SAVE_update_PATH}_latest.jpg')\n",
    "                plt.savefig(save_name_path)\n",
    "                plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313 (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
