{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Attack Type Generalization (CIFAR10)\n",
    "\n",
    "## Experiment Goal\n",
    "Verify that robustness findings **generalize** across different attack types beyond label flipping.\n",
    "\n",
    "## Research Questions\n",
    "1. Does width-based robustness work for different attack types?\n",
    "2. How do label flip vs random noise attacks compare in effectiveness?\n",
    "3. Is there attack transferability (defense against one helps against others)?\n",
    "4. Which attack is hardest to defend against?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Styling\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully: 2 rows\n"
     ]
    }
   ],
   "source": [
    "RESULT_FILE = \"./results_exp4_cifar10/final_results.csv\"\n",
    "OUTPUT_DIR = \"./results_exp4_cifar10/plots\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(RESULT_FILE)\n",
    "    print(f\"✓ Data loaded successfully: {len(df)} rows\")\n",
    "    df.head()\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Error: File {RESULT_FILE} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (2, 20)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'poison_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'poison_type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset Shape:\u001b[39m\u001b[33m\"\u001b[39m, df.shape)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAttack Types:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpoison_type\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.unique())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWidth Factors:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mwidth_factor\u001b[39m\u001b[33m'\u001b[39m].unique()))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPoison Ratios:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mpoison_ratio\u001b[39m\u001b[33m'\u001b[39m].unique()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\github\\poisoning_attack_analysis\\env313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'poison_type'"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nAttack Types:\", df['poison_type'].unique())\n",
    "print(\"Width Factors:\", sorted(df['width_factor'].unique()))\n",
    "print(\"Poison Ratios:\", sorted(df['poison_ratio'].unique()))\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Multi-Panel: Attack Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each attack type\n",
    "attack_types = df['poison_type'].unique()\n",
    "fig, axes = plt.subplots(1, len(attack_types), figsize=(7*len(attack_types), 6))\n",
    "\n",
    "if len(attack_types) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, attack in enumerate(attack_types):\n",
    "    subset = df[df['poison_type'] == attack]\n",
    "    \n",
    "    # Plot for each poison ratio\n",
    "    poison_ratios = sorted(subset['poison_ratio'].unique())\n",
    "    colors = sns.color_palette(\"viridis\", len(poison_ratios))\n",
    "    \n",
    "    for i, pr in enumerate(poison_ratios):\n",
    "        pr_data = subset[subset['poison_ratio'] == pr].sort_values('width_factor')\n",
    "        axes[idx].plot(pr_data['width_factor'], pr_data['mean_test_acc'],\n",
    "                       marker='o', linewidth=2.5, color=colors[i],\n",
    "                       label=f'Poison {pr}')\n",
    "        axes[idx].fill_between(\n",
    "            pr_data['width_factor'],\n",
    "            pr_data['mean_test_acc'] - pr_data['std_test_acc'],\n",
    "            pr_data['mean_test_acc'] + pr_data['std_test_acc'],\n",
    "            color=colors[i], alpha=0.2\n",
    "        )\n",
    "    \n",
    "    axes[idx].set_title(f'Attack: {attack}', fontsize=14, weight='bold')\n",
    "    axes[idx].set_xlabel('Width Factor', fontsize=12)\n",
    "    axes[idx].set_ylabel('Test Accuracy', fontsize=12)\n",
    "    axes[idx].legend(loc='lower right', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].set_ylim(0, 1.05)\n",
    "\n",
    "plt.suptitle('Attack Type Generalization Analysis', fontsize=16, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/attack_type_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attack Effectiveness Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate attack effectiveness for each type\n",
    "effectiveness_data = []\n",
    "\n",
    "for attack in attack_types:\n",
    "    for width in df['width_factor'].unique():\n",
    "        for pr in [p for p in df['poison_ratio'].unique() if p > 0]:\n",
    "            clean_acc = df[(df['poison_type'] == attack) & \n",
    "                          (df['width_factor'] == width) & \n",
    "                          (df['poison_ratio'] == 0.0)]['mean_test_acc'].values\n",
    "            poisoned_acc = df[(df['poison_type'] == attack) & \n",
    "                             (df['width_factor'] == width) & \n",
    "                             (df['poison_ratio'] == pr)]['mean_test_acc'].values\n",
    "            \n",
    "            if len(clean_acc) > 0 and len(poisoned_acc) > 0:\n",
    "                effectiveness = (1 - poisoned_acc[0] / clean_acc[0]) * 100\n",
    "                effectiveness_data.append({\n",
    "                    'attack_type': attack,\n",
    "                    'width_factor': width,\n",
    "                    'poison_ratio': pr,\n",
    "                    'effectiveness': effectiveness\n",
    "                })\n",
    "\n",
    "eff_df = pd.DataFrame(effectiveness_data)\n",
    "\n",
    "# Plot\n",
    "if len(eff_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=eff_df,\n",
    "        x='poison_ratio',\n",
    "        y='effectiveness',\n",
    "        hue='attack_type',\n",
    "        palette='Set2',\n",
    "        edgecolor='black',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Attack Effectiveness Comparison', fontsize=16, weight='bold')\n",
    "    ax.set_xlabel('Poison Ratio', fontsize=14)\n",
    "    ax.set_ylabel('Attack Effectiveness (%)', fontsize=14)\n",
    "    ax.legend(title='Attack Type', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/attack_effectiveness_comparison.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cross-Attack Transferability Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between attack performances\n",
    "if len(attack_types) > 1:\n",
    "    # Create pivot for each attack type\n",
    "    attack_pivots = {}\n",
    "    for attack in attack_types:\n",
    "        subset = df[df['poison_type'] == attack]\n",
    "        # Create unique key for matching\n",
    "        subset['key'] = (subset['width_factor'].astype(str) + '_' + \n",
    "                        subset['poison_ratio'].astype(str))\n",
    "        attack_pivots[attack] = subset.set_index('key')['mean_test_acc']\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_df = pd.DataFrame(attack_pivots)\n",
    "    correlation_matrix = corr_df.corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.3f', \n",
    "                cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "                square=True, linewidths=1)\n",
    "    plt.title('Cross-Attack Transferability Matrix\\n(Correlation of Performance)', \n",
    "              fontsize=14, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/transferability_matrix.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCorrelation Interpretation:\")\n",
    "    print(\"  High correlation (>0.7): defenses transfer well between attacks\")\n",
    "    print(\"  Low correlation (<0.3): attacks require different defenses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. General Observations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Attack Type Comparison**\n",
    "   - Different attack types may have varying effectiveness\n",
    "   - Label flipping: targeted, predictable corruption\n",
    "   - Random noise: untargeted, stochastic corruption\n",
    "\n",
    "2. **Robustness Generalization**\n",
    "   - Do wider models resist all attack types equally?\n",
    "   - Or are some attacks more sensitive to model capacity?\n",
    "\n",
    "3. **Defense Transferability**\n",
    "   - High correlation: universal defense (works for all attacks)\n",
    "   - Low correlation: attack-specific vulnerabilities exist\n",
    "\n",
    "4. **Hardest Attack**\n",
    "   - Which attack causes most accuracy degradation?\n",
    "   - Guides prioritization of defense mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mathematical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Cross-Attack Generalization Metric\n",
    "\n",
    "Generalization score $G$ measures correlation between attack vulnerabilities:\n",
    "\n",
    "$$\n",
    "G(a_1, a_2) = \\text{Corr}\\left(\\text{Acc}_{a_1}(w, p), \\text{Acc}_{a_2}(w, p)\\right)\n",
    "$$\n",
    "\n",
    "where $a_1, a_2$ are different attack types, $w$ is width, $p$ is poison ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Attack Strength Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average attack strength for each type\n",
    "if len(eff_df) > 0:\n",
    "    print(\"Attack Strength Analysis:\\n\")\n",
    "    \n",
    "    for attack in attack_types:\n",
    "        attack_data = eff_df[eff_df['attack_type'] == attack]\n",
    "        \n",
    "        print(f\"\\n{attack.upper()}:\")\n",
    "        print(f\"  Mean Effectiveness: {attack_data['effectiveness'].mean():.2f}%\")\n",
    "        print(f\"  Max Effectiveness: {attack_data['effectiveness'].max():.2f}%\")\n",
    "        print(f\"  Std Dev: {attack_data['effectiveness'].std():.2f}%\")\n",
    "    \n",
    "    # Find hardest attack\n",
    "    attack_strengths = eff_df.groupby('attack_type')['effectiveness'].mean()\n",
    "    hardest_attack = attack_strengths.idxmax()\n",
    "    print(f\"\\nHardest Attack to Defend: {hardest_attack}\")\n",
    "    print(f\"  Average Effectiveness: {attack_strengths[hardest_attack]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Statistical Comparison Between Attack Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attack types statistically\n",
    "if len(attack_types) > 1:\n",
    "    print(\"\\nPairwise Statistical Comparison:\\n\")\n",
    "    \n",
    "    for pr in [p for p in df['poison_ratio'].unique() if p > 0]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Poison Ratio: {pr}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        attack_list = list(attack_types)\n",
    "        for i in range(len(attack_list)):\n",
    "            for j in range(i+1, len(attack_list)):\n",
    "                attack1 = attack_list[i]\n",
    "                attack2 = attack_list[j]\n",
    "                \n",
    "                data1 = df[(df['poison_type'] == attack1) & \n",
    "                          (df['poison_ratio'] == pr)]['mean_test_acc'].values\n",
    "                data2 = df[(df['poison_type'] == attack2) & \n",
    "                          (df['poison_ratio'] == pr)]['mean_test_acc'].values\n",
    "                \n",
    "                if len(data1) > 0 and len(data2) > 0:\n",
    "                    t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "                    \n",
    "                    print(f\"\\n{attack1} vs {attack2}:\")\n",
    "                    print(f\"  {attack1}: {data1.mean():.4f} ± {data1.std():.4f}\")\n",
    "                    print(f\"  {attack2}: {data2.mean():.4f} ± {data2.std():.4f}\")\n",
    "                    print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "                    \n",
    "                    if p_value < 0.05:\n",
    "                        winner = attack1 if data1.mean() > data2.mean() else attack2\n",
    "                        print(f\"  ✓ {winner} shows better robustness (p < 0.05)\")\n",
    "                    else:\n",
    "                        print(\"  ✗ No significant difference (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Generalization of Width-Based Defense**\n",
    "   - Confirms whether robustness from wider models applies across attack types\n",
    "   - Universal defense vs attack-specific vulnerabilities\n",
    "\n",
    "2. **Attack Transferability**\n",
    "   - High correlation: defense mechanisms generalize well\n",
    "   - Low correlation: need diverse defense portfolio\n",
    "\n",
    "3. **Most Challenging Attack**\n",
    "   - Identifies which attack type is hardest to defend against\n",
    "   - Prioritizes research and defense development\n",
    "\n",
    "4. **Practical Deployment Insights**\n",
    "   - If defenses generalize: single defense strategy sufficient\n",
    "   - If attack-specific: need adaptive/ensemble defenses\n",
    "\n",
    "5. **Research Implications**\n",
    "   - validates robustness findings across multiple threat models\n",
    "   - Broader applicability of architectural defenses\n",
    "   - Future: test on more sophisticated attacks (backdoors, gradient-based)\n",
    "\n",
    "### Final Summary\n",
    "\n",
    "This experiment completes the comprehensive analysis by verifying that findings generalize beyond a single attack type. The transferability analysis reveals whether we've discovered universal robustness principles or attack-specific phenomena.\n",
    "\n",
    "---\n",
    "\n",
    "**Complete Analysis Overview**: \n",
    "- **Exp0**: Width vs Depth trade-offs\n",
    "- **Exp1**: Double descent and over-parameterization benefits\n",
    "- **Exp2**: Intrinsic (width) vs Extrinsic (aggregation) defenses\n",
    "- **Exp3**: Mechanism analysis (batch size, ordering)\n",
    "- **Exp4**: Attack type generalization (current)\n",
    "\n",
    "**See CIFAR10 notebooks for validation on more complex datasets.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
