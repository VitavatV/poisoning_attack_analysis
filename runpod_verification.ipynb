{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunPod RTX 3090 Setup Verification\n",
    "\n",
    "This notebook verifies your RunPod environment is properly configured for the poisoning attack analysis experiments.\n",
    "\n",
    "## Hardware Specs\n",
    "- **GPU**: RTX 3090 (24GB VRAM)\n",
    "- **CPU**: 32 cores\n",
    "- **RAM**: 125GB\n",
    "\n",
    "Run all cells to verify your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA Available: {cuda.is_available()}\")\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU Name: {cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {cuda.current_device()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    mem_total = cuda.get_device_properties(0).total_memory / 1e9\n",
    "    mem_allocated = cuda.memory_allocated(0) / 1e9\n",
    "    mem_reserved = cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    print(f\"\\nMemory Info:\")\n",
    "    print(f\"  Total VRAM: {mem_total:.2f} GB\")\n",
    "    print(f\"  Allocated: {mem_allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {mem_reserved:.2f} GB\")\n",
    "    print(f\"  Free: {mem_total - mem_reserved:.2f} GB\")\n",
    "    \n",
    "    # Check if it's RTX 3090\n",
    "    if \"3090\" in cuda.get_device_name(0):\n",
    "        print(\"\\n‚úÖ RTX 3090 detected!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Expected RTX 3090, but found {cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is not available! PyTorch will run on CPU.\")\n",
    "    print(\"Please check your CUDA installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check CPU and RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CPU & RAM INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CPU info\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "\n",
    "# RAM info\n",
    "ram_total = psutil.virtual_memory().total / 1e9\n",
    "ram_available = psutil.virtual_memory().available / 1e9\n",
    "ram_percent = psutil.virtual_memory().percent\n",
    "\n",
    "print(f\"\\nRAM Info:\")\n",
    "print(f\"  Total RAM: {ram_total:.2f} GB\")\n",
    "print(f\"  Available RAM: {ram_available:.2f} GB\")\n",
    "print(f\"  Used: {ram_percent:.1f}%\")\n",
    "\n",
    "# Verify specs\n",
    "if cpu_count >= 32:\n",
    "    print(f\"\\n‚úÖ CPU cores: {cpu_count} (expected ~32)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è CPU cores: {cpu_count} (expected 32)\")\n",
    "\n",
    "if ram_total >= 120:\n",
    "    print(f\"‚úÖ RAM: {ram_total:.0f} GB (expected ~125 GB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è RAM: {ram_total:.0f} GB (expected 125 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test PyTorch Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEPENDENCY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dependencies = [\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'yaml'\n",
    "]\n",
    "\n",
    "import importlib\n",
    "\n",
    "for dep in dependencies:\n",
    "    try:\n",
    "        if dep == 'yaml':\n",
    "            module = importlib.import_module(dep)\n",
    "        else:\n",
    "            module = importlib.import_module(dep)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {dep:15s} version: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {dep:15s} NOT INSTALLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Project Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project directory to path (adjust if needed)\n",
    "project_path = '/workspace/poisoning_attack_analysis'  # Change this to your project path\n",
    "if os.path.exists(project_path):\n",
    "    sys.path.insert(0, project_path)\n",
    "    print(f\"‚úÖ Project path added: {project_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Project path not found: {project_path}\")\n",
    "    print(\"Please update the project_path variable above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT MODULE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from models import ScalableCNN\n",
    "    print(\"‚úÖ models.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import models.py: {e}\")\n",
    "\n",
    "try:\n",
    "    from data_utils import load_global_dataset, partition_data_dirichlet\n",
    "    print(\"‚úÖ data_utils.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import data_utils.py: {e}\")\n",
    "\n",
    "try:\n",
    "    from utils import train_client, evaluate_model, fed_avg\n",
    "    print(\"‚úÖ utils.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import utils.py: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick GPU Test (Training a Small Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU TRAINING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1000, 5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000, 5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000, 10)\n",
    ").to(device)\n",
    "\n",
    "# Create dummy data\n",
    "x = torch.randn(128, 1000).to(device)\n",
    "y = torch.randint(0, 10, (128,)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nRunning 100 training iterations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Iteration {i+1}/100, Loss: {loss.item():.4f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {elapsed:.2f} seconds\")\n",
    "print(f\"   Average: {elapsed/100*1000:.2f} ms per iteration\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration File Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIG FILE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "config_dir = os.path.join(project_path, 'configs')\n",
    "config_files = [\n",
    "    'config_exp0_mnist.yaml',\n",
    "    'config_exp0_cifar10.yaml',\n",
    "    'config_exp1_mnist.yaml',\n",
    "    'config_exp1_cifar10.yaml',\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = os.path.join(config_dir, config_file)\n",
    "    if os.path.exists(config_path):\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            batch_size = config['defaults']['batch_size']\n",
    "            num_workers = config['defaults'].get('num_parallel_workers', 'not set')\n",
    "            print(f\"‚úÖ {config_file:30s} batch_size={batch_size}, workers={num_workers}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {config_file:30s} Error: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {config_file:30s} NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SETUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = {\n",
    "    \"GPU (CUDA)\": torch.cuda.is_available(),\n",
    "    \"RTX 3090\": \"3090\" in torch.cuda.get_device_name(0) if torch.cuda.is_available() else False,\n",
    "    \"CPU Cores (‚â•32)\": multiprocessing.cpu_count() >= 32,\n",
    "    \"RAM (‚â•120 GB)\": psutil.virtual_memory().total / 1e9 >= 120,\n",
    "}\n",
    "\n",
    "for check_name, status in checks.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{status_icon} {check_name}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ ALL CHECKS PASSED! You're ready to run experiments!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Navigate to your project directory\")\n",
    "    print(\"2. Run a test experiment:\")\n",
    "    print(\"   python experiment_runner.py configs/config_exp1_mnist.yaml\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some checks failed. Please review the output above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
