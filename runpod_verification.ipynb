{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunPod RTX 3090 Setup Verification\n",
    "\n",
    "This notebook verifies your RunPod environment is properly configured for the poisoning attack analysis experiments.\n",
    "\n",
    "## Hardware Specs\n",
    "- **GPU**: RTX 3090 (24GB VRAM)\n",
    "- **CPU**: 32 cores\n",
    "- **RAM**: 125GB\n",
    "\n",
    "Run all cells to verify your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU INFORMATION\n",
      "============================================================\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA RTX 2000 Ada Generation\n",
      "CUDA Version: 12.8\n",
      "GPU Count: 3\n",
      "Current GPU: 0\n",
      "\n",
      "Memory Info:\n",
      "  Total VRAM: 16.73 GB\n",
      "  Allocated: 0.00 GB\n",
      "  Reserved: 0.00 GB\n",
      "  Free: 16.73 GB\n",
      "\n",
      "‚úÖ RTX 2000 detected!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.cuda as cuda\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA Available: {cuda.is_available()}\")\n",
    "\n",
    "if cuda.is_available():\n",
    "    print(f\"GPU Name: {cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Count: {cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {cuda.current_device()}\")\n",
    "    \n",
    "    # Memory info\n",
    "    mem_total = cuda.get_device_properties(0).total_memory / 1e9\n",
    "    mem_allocated = cuda.memory_allocated(0) / 1e9\n",
    "    mem_reserved = cuda.memory_reserved(0) / 1e9\n",
    "    \n",
    "    print(f\"\\nMemory Info:\")\n",
    "    print(f\"  Total VRAM: {mem_total:.2f} GB\")\n",
    "    print(f\"  Allocated: {mem_allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {mem_reserved:.2f} GB\")\n",
    "    print(f\"  Free: {mem_total - mem_reserved:.2f} GB\")\n",
    "    \n",
    "    # Check if it's RTX 2000\n",
    "    if \"2000\" in cuda.get_device_name(0):\n",
    "        print(\"\\n‚úÖ RTX 2000 detected!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Warning: Expected RTX 3090, but found {cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA is not available! PyTorch will run on CPU.\")\n",
    "    print(\"Please check your CUDA installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check CPU and RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CPU & RAM INFORMATION\n",
      "============================================================\n",
      "CPU Cores: 48\n",
      "\n",
      "RAM Info:\n",
      "  Total RAM: 270.09 GB\n",
      "  Available RAM: 242.92 GB\n",
      "  Used: 10.1%\n",
      "\n",
      "‚úÖ CPU cores: 48 (expected ~32)\n",
      "‚úÖ RAM: 270 GB (expected ~125 GB)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CPU & RAM INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CPU info\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"CPU Cores: {cpu_count}\")\n",
    "\n",
    "# RAM info\n",
    "ram_total = psutil.virtual_memory().total / 1e9\n",
    "ram_available = psutil.virtual_memory().available / 1e9\n",
    "ram_percent = psutil.virtual_memory().percent\n",
    "\n",
    "print(f\"\\nRAM Info:\")\n",
    "print(f\"  Total RAM: {ram_total:.2f} GB\")\n",
    "print(f\"  Available RAM: {ram_available:.2f} GB\")\n",
    "print(f\"  Used: {ram_percent:.1f}%\")\n",
    "\n",
    "# Verify specs\n",
    "if cpu_count >= 32:\n",
    "    print(f\"\\n‚úÖ CPU cores: {cpu_count} (expected ~32)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è CPU cores: {cpu_count} (expected 32)\")\n",
    "\n",
    "if ram_total >= 120:\n",
    "    print(f\"‚úÖ RAM: {ram_total:.0f} GB (expected ~125 GB)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è RAM: {ram_total:.0f} GB (expected 125 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test PyTorch Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEPENDENCY CHECK\n",
      "============================================================\n",
      "‚úÖ torch           version: 2.8.0+cu128\n",
      "‚úÖ torchvision     version: 0.23.0+cu128\n",
      "‚úÖ numpy           version: 2.1.2\n",
      "‚úÖ pandas          version: 2.3.3\n",
      "‚úÖ matplotlib      version: 3.10.7\n",
      "‚úÖ seaborn         version: 0.13.2\n",
      "‚úÖ yaml            version: 6.0.3\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEPENDENCY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dependencies = [\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'yaml'\n",
    "]\n",
    "\n",
    "import importlib\n",
    "\n",
    "for dep in dependencies:\n",
    "    try:\n",
    "        if dep == 'yaml':\n",
    "            module = importlib.import_module(dep)\n",
    "        else:\n",
    "            module = importlib.import_module(dep)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {dep:15s} version: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {dep:15s} NOT INSTALLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Project Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project path added: /workspace/poisoning_attack_analysis\n",
      "\n",
      "============================================================\n",
      "PROJECT MODULE CHECK\n",
      "============================================================\n",
      "‚úÖ models.py imported successfully\n",
      "‚úÖ data_utils.py imported successfully\n",
      "‚úÖ utils.py imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project directory to path (adjust if needed)\n",
    "project_path = '/workspace/poisoning_attack_analysis'  # Change this to your project path\n",
    "if os.path.exists(project_path):\n",
    "    sys.path.insert(0, project_path)\n",
    "    print(f\"‚úÖ Project path added: {project_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Project path not found: {project_path}\")\n",
    "    print(\"Please update the project_path variable above.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT MODULE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from models import ScalableCNN\n",
    "    print(\"‚úÖ models.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import models.py: {e}\")\n",
    "\n",
    "try:\n",
    "    from data_utils import load_global_dataset, partition_data_dirichlet\n",
    "    print(\"‚úÖ data_utils.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import data_utils.py: {e}\")\n",
    "\n",
    "try:\n",
    "    from utils import train_client, evaluate_model, fed_avg\n",
    "    print(\"‚úÖ utils.py imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import utils.py: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick GPU Test (Training a Small Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU TRAINING TEST\n",
      "============================================================\n",
      "Using device: cuda\n",
      "\n",
      "Running 100 training iterations...\n",
      "  Iteration 20/100, Loss: 2.0146\n",
      "  Iteration 40/100, Loss: 1.7329\n",
      "  Iteration 60/100, Loss: 1.4475\n",
      "  Iteration 80/100, Loss: 1.1622\n",
      "  Iteration 100/100, Loss: 0.8946\n",
      "\n",
      "‚úÖ Training completed in 0.67 seconds\n",
      "   Average: 6.75 ms per iteration\n",
      "\n",
      "GPU Memory Usage:\n",
      "  Allocated: 0.26 GB\n",
      "  Reserved: 0.29 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU TRAINING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a simple model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1000, 5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000, 5000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5000, 10)\n",
    ").to(device)\n",
    "\n",
    "# Create dummy data\n",
    "x = torch.randn(128, 1000).to(device)\n",
    "y = torch.randint(0, 10, (128,)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nRunning 100 training iterations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Iteration {i+1}/100, Loss: {loss.item():.4f}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {elapsed:.2f} seconds\")\n",
    "print(f\"   Average: {elapsed/100*1000:.2f} ms per iteration\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration File Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIG FILE CHECK\n",
      "============================================================\n",
      "‚úÖ config_exp0_mnist.yaml         batch_size=128, workers=1\n",
      "‚úÖ config_exp0_cifar10.yaml       batch_size=128, workers=1\n",
      "‚úÖ config_exp1_mnist.yaml         batch_size=128, workers=1\n",
      "‚úÖ config_exp1_cifar10.yaml       batch_size=128, workers=1\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIG FILE CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "config_dir = os.path.join(project_path, 'configs')\n",
    "config_files = [\n",
    "    'config_exp0_mnist.yaml',\n",
    "    'config_exp0_cifar10.yaml',\n",
    "    'config_exp1_mnist.yaml',\n",
    "    'config_exp1_cifar10.yaml',\n",
    "]\n",
    "\n",
    "for config_file in config_files:\n",
    "    config_path = os.path.join(config_dir, config_file)\n",
    "    if os.path.exists(config_path):\n",
    "        try:\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            batch_size = config['defaults']['batch_size']\n",
    "            num_workers = config['defaults'].get('num_parallel_workers', 'not set')\n",
    "            print(f\"‚úÖ {config_file:30s} batch_size={batch_size}, workers={num_workers}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {config_file:30s} Error: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {config_file:30s} NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SETUP SUMMARY\n",
      "============================================================\n",
      "‚úÖ GPU (CUDA)\n",
      "‚úÖ RTX 2000\n",
      "‚úÖ CPU Cores (‚â•32)\n",
      "‚úÖ RAM (‚â•120 GB)\n",
      "\n",
      "============================================================\n",
      "üéâ ALL CHECKS PASSED! You're ready to run experiments!\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "1. Navigate to your project directory\n",
      "2. Run a test experiment:\n",
      "   python experiment_runner.py configs/config_exp1_mnist.yaml\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SETUP SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checks = {\n",
    "    \"GPU (CUDA)\": torch.cuda.is_available(),\n",
    "    \"RTX 2000\": \"2000\" in torch.cuda.get_device_name(0) if torch.cuda.is_available() else False,\n",
    "    \"CPU Cores (‚â•32)\": multiprocessing.cpu_count() >= 32,\n",
    "    \"RAM (‚â•120 GB)\": psutil.virtual_memory().total / 1e9 >= 120,\n",
    "}\n",
    "\n",
    "for check_name, status in checks.items():\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{status_icon} {check_name}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ ALL CHECKS PASSED! You're ready to run experiments!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Navigate to your project directory\")\n",
    "    print(\"2. Run a test experiment:\")\n",
    "    print(\"   python experiment_runner.py configs/config_exp1_mnist.yaml\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some checks failed. Please review the output above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
